{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(start_path: Path = Path.cwd(), marker: str = 'pyproject.toml') -> Path:\n",
    "    current_path = start_path.resolve()\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "        \n",
    "def add_project_root_to_sys_path(marker: str = 'pyproject.toml'):\n",
    "    project_root = find_project_root(marker=marker)\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "\n",
    "add_project_root_to_sys_path()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from src.asym_ensembles.data_loaders import load_dataset\n",
    "from src.asym_ensembles.modeling.training import (\n",
    "    set_global_seed,\n",
    "    train_one_model,\n",
    "    evaluate_model,\n",
    "    evaluate_ensemble,\n",
    "    average_pairwise_distance\n",
    ")\n",
    "from src.asym_ensembles.modeling.models import MLP, WMLP\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg={\n",
    "    \"batch_size\": 64,\n",
    "    \"max_epochs\": 200,\n",
    "    \"patience\": 16,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 3e-2,\n",
    "    \"hidden_dims\": [64, 128, 256],\n",
    "    \"ensemble_sizes\": [2, 4, 8, 16, 32, 64],\n",
    "    \"total_models\": 64,             # max(ensemble_sizes)\n",
    "    \"repeats\": 10,                  # different seeds\n",
    "    \"mask_type\": \"random_subsets\",\n",
    "    \"base_seed\": 1234,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"DeepEnsembleProject\", config=cfg, name=\"Extended_Experiments\", settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = wandb.config\n",
    "table1 = wandb.Table(\n",
    "    columns=[\"dataset_name\", \"hidden_dim\", \"repeat_index\", \"model_index\", \"metric_type\", \"metric\", \"masked_ratio\"]\n",
    ")\n",
    "\n",
    "table2 = wandb.Table(\n",
    "    columns=[\n",
    "        \"dataset_name\", \"hidden_dim\", \"repeat_index\", \"metric_type\",\n",
    "        \"avg_dist_mlp\", \"avg_dist_wmlp\",\n",
    "        \"mean_mlp_metric\", \"std_mlp_metric\", \"min_mlp_metric\", \"max_mlp_metric\",\n",
    "        \"mean_wmlp_metric\", \"std_wmlp_metric\", \"min_wmlp_metric\", \"max_wmlp_metric\",\n",
    "        \"ens_size\", \"mlp_ens_metric\", \"wmlp_ens_metric\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\n",
    "    (\"california\", \"regression\"),\n",
    "    (\"otto\", \"classification\"),\n",
    "    (\"telcom\", \"classification\"),\n",
    "    (\"mnist\", \"classification\"),\n",
    "]\n",
    "for dataset_name, task_type in all_datasets:\n",
    "    train_ds, val_ds, test_ds = load_dataset(dataset_name=dataset_name)\n",
    "    train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=config.batch_size, shuffle=False)\n",
    "    for hidden_dim in config.hidden_dims:\n",
    "        print(f\"\\nDataset: {dataset_name}, Hidden_dim: {hidden_dim}\")\n",
    "        if hidden_dim in [64, 128]:\n",
    "            second_nfix = 3\n",
    "        else:\n",
    "            second_nfix = 4\n",
    "        mask_params = {\n",
    "            0: {'mask_constant': 1, 'mask_type': config.mask_type, 'do_normal_mask': True, 'num_fixed': 2},\n",
    "            1: {'mask_constant': 1, 'mask_type': config.mask_type, 'do_normal_mask': True, 'num_fixed': second_nfix},\n",
    "            2: {'mask_constant': 1, 'mask_type': config.mask_type, 'do_normal_mask': True, 'num_fixed': second_nfix},\n",
    "            3: {'mask_constant': 1, 'mask_type': config.mask_type, 'do_normal_mask': True, 'num_fixed': second_nfix},\n",
    "        }\n",
    "        in_features = train_ds.tensors[0].shape[1]\n",
    "        if task_type == \"regression\":\n",
    "            out_features = 1\n",
    "            criterion = nn.MSELoss()\n",
    "            metric_type = \"rmse\"\n",
    "        else:\n",
    "            out_features = len(torch.unique(train_ds.tensors[1]))\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            metric_type = \"accuracy\"\n",
    "\n",
    "        for rep_i in range(config.repeats):\n",
    "            print(f\"\\nRepetition {rep_i + 1}/{config.repeats}\")\n",
    "            current_seed = config.base_seed + rep_i * 10000\n",
    "\n",
    "            mlp_metrics = []\n",
    "            wmlp_metrics = []\n",
    "            wmlp_masked_ratios = []\n",
    "            mlp_models = []\n",
    "            wmlp_models = []\n",
    "\n",
    "            for i in tqdm(range(config.total_models), desc=\"Training MLP\"):\n",
    "                seed_value = current_seed + i\n",
    "                set_global_seed(seed_value)\n",
    "\n",
    "                mlp = MLP(in_features, hidden_dim, out_features, num_layers=4, norm=None)\n",
    "                optimizer = torch.optim.AdamW(mlp.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "                mlp, train_time, train_losses, val_losses = train_one_model(\n",
    "                    mlp, train_loader, val_loader, criterion, optimizer,\n",
    "                    device=config.device, max_epochs=config.max_epochs, patience=config.patience\n",
    "                )\n",
    "                mlp.to(\"cpu\")\n",
    "                mlp_models.append(copy.deepcopy(mlp))\n",
    "\n",
    "                test_metric = evaluate_model(mlp, test_loader, criterion, config.device, task_type=task_type)\n",
    "                mlp_metrics.append(test_metric)\n",
    "                table1.add_data(\n",
    "                    dataset_name,\n",
    "                    hidden_dim,\n",
    "                    rep_i + 1,\n",
    "                    i + 1,\n",
    "                    metric_type,\n",
    "                    test_metric,\n",
    "                    0\n",
    "                )\n",
    "\n",
    "            for i in tqdm(range(config.total_models), desc=\"Training WMLP\"):\n",
    "                seed_value_wmlp = current_seed + 2000 + i\n",
    "                set_global_seed(seed_value_wmlp)\n",
    "\n",
    "                wmlp = WMLP(in_features, hidden_dim, out_features, num_layers=4, mask_params=mask_params, norm=None)\n",
    "                optimizer_wmlp = torch.optim.AdamW(wmlp.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "                wmlp, train_time_wmlp, train_losses_w, val_losses_w = train_one_model(\n",
    "                    wmlp, train_loader, val_loader, criterion, optimizer_wmlp,\n",
    "                    device=config.device, max_epochs=config.max_epochs, patience=config.patience\n",
    "                )\n",
    "                wmlp.to(\"cpu\")\n",
    "                wmlp_models.append(copy.deepcopy(wmlp))\n",
    "                test_metric_wmlp = evaluate_model(wmlp, test_loader, criterion, config.device, task_type=task_type)\n",
    "                wmlp_metrics.append(test_metric_wmlp)\n",
    "                ratio, masked = wmlp.report_masked_ratio()\n",
    "                wmlp_masked_ratios.append(ratio)\n",
    "                table1.add_data(\n",
    "                    dataset_name,\n",
    "                    hidden_dim,\n",
    "                    rep_i + 1,\n",
    "                    i + 1,\n",
    "                    metric_type,\n",
    "                    test_metric,\n",
    "                    ratio\n",
    "                )\n",
    "\n",
    "            avg_dist_mlp = average_pairwise_distance(mlp_models)\n",
    "            avg_dist_wmlp = average_pairwise_distance(wmlp_models)\n",
    "\n",
    "            avg_wmlp_masked_ratio = float(np.mean(wmlp_masked_ratios)) if wmlp_masked_ratios else 0.0\n",
    "\n",
    "            mean_mlp_metric = float(np.mean(mlp_metrics))\n",
    "            std_mlp_metric = float(np.std(mlp_metrics))\n",
    "            min_mlp_metric = float(np.min(mlp_metrics))\n",
    "            max_mlp_metric = float(np.max(mlp_metrics))\n",
    "\n",
    "            mean_wmlp_metric = float(np.mean(wmlp_metrics))\n",
    "            std_wmlp_metric = float(np.std(wmlp_metrics))\n",
    "            min_wmlp_metric = float(np.min(wmlp_metrics))\n",
    "            max_wmlp_metric = float(np.max(wmlp_metrics))\n",
    "\n",
    "\n",
    "            ensemble_results_mlp = {}\n",
    "            ensemble_results_wmlp = {}\n",
    "            for ens_size in config.ensemble_sizes:\n",
    "                mlp_sub = mlp_models[:ens_size]\n",
    "                wmlp_sub = wmlp_models[:ens_size]\n",
    "                ens_metric_mlp = evaluate_ensemble(mlp_sub, test_loader, config.device, task_type=task_type)\n",
    "                ens_metric_wmlp = evaluate_ensemble(wmlp_sub, test_loader, config.device, task_type=task_type)\n",
    "                ensemble_results_mlp[ens_size] = ens_metric_mlp\n",
    "                ensemble_results_wmlp[ens_size] = ens_metric_wmlp\n",
    "\n",
    "            for ens_size in config.ensemble_sizes:\n",
    "                table2.add_data(\n",
    "                    dataset_name,\n",
    "                    hidden_dim,\n",
    "                    rep_i + 1,\n",
    "                    metric_type,\n",
    "                    avg_dist_mlp,\n",
    "                    avg_dist_wmlp,\n",
    "                    mean_mlp_metric,\n",
    "                    std_mlp_metric,\n",
    "                    min_mlp_metric,\n",
    "                    max_mlp_metric,\n",
    "                    mean_wmlp_metric,\n",
    "                    std_wmlp_metric,\n",
    "                    min_wmlp_metric,\n",
    "                    max_wmlp_metric,\n",
    "                    ens_size,\n",
    "                    ensemble_results_mlp[ens_size],\n",
    "                    ensemble_results_wmlp[ens_size]\n",
    "                )\n",
    "\n",
    "            print(f\"Repetition {rep_i + 1}/{config.repeats} finished.\")\n",
    "            break\n",
    "        break\n",
    "\n",
    "wandb.log({\n",
    "    \"Model Metrics\": table1,\n",
    "    \"Aggregate Metrics\": table2\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asym-ensembles-Bo8XDwk--py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
