{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(start_path: Path = Path.cwd(), marker: str = 'pyproject.toml') -> Path:\n",
    "    current_path = start_path.resolve()\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "        \n",
    "def add_project_root_to_sys_path(marker: str = 'pyproject.toml'):\n",
    "    project_root = find_project_root(marker=marker)\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "\n",
    "add_project_root_to_sys_path()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from src.asym_ensembles.data_loaders import load_dataset\n",
    "from src.asym_ensembles.modeling.training import (\n",
    "    set_global_seed,\n",
    "    train_one_model,\n",
    "    evaluate_model,\n",
    "    evaluate_ensemble,\n",
    "    average_pairwise_distance\n",
    ")\n",
    "from src.asym_ensembles.modeling.models import MLP, WMLP\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg={\n",
    "    \"batch_size\": 64,\n",
    "    \"max_epochs\": 200,\n",
    "    \"patience\": 16,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 3e-2,\n",
    "    \"hidden_dims\": [64, 128, 256],\n",
    "    \"ensemble_sizes\": [2, 4, 8, 16, 32, 64],\n",
    "    \"total_models\": 64,             # max(ensemble_sizes)\n",
    "    \"repeats\": 10,                  # different seeds\n",
    "    \"mask_type\": \"random_subsets\",\n",
    "    \"base_seed\": 1234,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnovitsk-oleg\u001b[0m (\u001b[33moanovi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/oanovitskij/Desktop/asym_ensembles/notebooks/wandb/run-20250122_231102-c4rzjcb7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oanovi/DeepEnsembleProject/runs/c4rzjcb7' target=\"_blank\">Extended_Experiments</a></strong> to <a href='https://wandb.ai/oanovi/DeepEnsembleProject' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oanovi/DeepEnsembleProject' target=\"_blank\">https://wandb.ai/oanovi/DeepEnsembleProject</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oanovi/DeepEnsembleProject/runs/c4rzjcb7' target=\"_blank\">https://wandb.ai/oanovi/DeepEnsembleProject/runs/c4rzjcb7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"DeepEnsembleProject\", config=cfg, name=\"Extended_Experiments\", settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: california, Hidden_dim: 64\n",
      "\n",
      "Repetition 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 101/200 [00:21<00:20,  4.73it/s]\n",
      "Training MLP:   2%|▏         | 1/64 [00:22<23:15, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|█████▉    | 119/200 [00:25<00:17,  4.72it/s]\n",
      "Training MLP:   3%|▎         | 2/64 [00:47<24:46, 23.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  56%|█████▌    | 111/200 [00:24<00:20,  4.45it/s]\n",
      "Training MLP:   5%|▍         | 3/64 [01:12<24:50, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  51%|█████     | 102/200 [00:21<00:20,  4.75it/s]\n",
      "Training MLP:   6%|▋         | 4/64 [01:33<23:16, 23.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  66%|██████▌   | 131/200 [00:27<00:14,  4.78it/s]\n",
      "Training MLP:   8%|▊         | 5/64 [02:01<24:21, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 100/200 [00:21<00:21,  4.55it/s]\n",
      "Training MLP:   9%|▉         | 6/64 [02:23<23:02, 23.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  59%|█████▉    | 118/200 [00:25<00:17,  4.71it/s]\n",
      "Training MLP:  11%|█         | 7/64 [02:48<23:02, 24.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  62%|██████▎   | 125/200 [00:27<00:16,  4.62it/s]\n",
      "Training MLP:  12%|█▎        | 8/64 [03:15<23:28, 25.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  64%|██████▎   | 127/200 [00:27<00:15,  4.59it/s]\n",
      "Training MLP:  14%|█▍        | 9/64 [03:43<23:46, 25.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  31%|███       | 62/200 [00:13<00:29,  4.70it/s]\n",
      "Training MLP:  16%|█▌        | 10/64 [03:56<19:48, 22.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  66%|██████▌   | 132/200 [00:27<00:14,  4.82it/s]\n",
      "Training MLP:  17%|█▋        | 11/64 [04:23<20:54, 23.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  41%|████      | 82/200 [00:17<00:25,  4.68it/s]\n",
      "Training MLP:  19%|█▉        | 12/64 [04:41<18:54, 21.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  63%|██████▎   | 126/200 [00:26<00:15,  4.71it/s]\n",
      "Training MLP:  20%|██        | 13/64 [05:08<19:48, 23.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  58%|█████▊    | 116/200 [00:26<00:18,  4.45it/s]\n",
      "Training MLP:  22%|██▏       | 14/64 [05:34<20:07, 24.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  72%|███████▏  | 143/200 [00:30<00:12,  4.62it/s]\n",
      "Training MLP:  23%|██▎       | 15/64 [06:05<21:23, 26.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  84%|████████▍ | 169/200 [00:35<00:06,  4.77it/s]\n",
      "Training MLP:  25%|██▌       | 16/64 [06:40<23:11, 28.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  48%|████▊     | 96/200 [00:21<00:23,  4.42it/s]\n",
      "Training MLP:  27%|██▋       | 17/64 [07:02<21:00, 26.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 134/200 [00:28<00:13,  4.75it/s]\n",
      "Training MLP:  28%|██▊       | 18/64 [07:30<20:53, 27.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  64%|██████▍   | 128/200 [00:26<00:14,  4.82it/s]\n",
      "Training MLP:  30%|██▉       | 19/64 [07:57<20:16, 27.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  51%|█████     | 102/200 [00:21<00:20,  4.77it/s]\n",
      "Training MLP:  31%|███▏      | 20/64 [08:18<18:35, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  57%|█████▋    | 114/200 [00:23<00:17,  4.88it/s]\n",
      "Training MLP:  33%|███▎      | 21/64 [08:42<17:44, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  36%|███▌      | 71/200 [00:15<00:28,  4.60it/s]\n",
      "Training MLP:  34%|███▍      | 22/64 [08:57<15:22, 21.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  68%|██████▊   | 137/200 [00:28<00:12,  4.87it/s]\n",
      "Training MLP:  36%|███▌      | 23/64 [09:25<16:16, 23.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  54%|█████▎    | 107/200 [00:22<00:19,  4.65it/s]\n",
      "Training MLP:  38%|███▊      | 24/64 [09:48<15:43, 23.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  76%|███████▌  | 151/200 [00:31<00:10,  4.73it/s]\n",
      "Training MLP:  39%|███▉      | 25/64 [10:20<16:57, 26.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  61%|██████    | 122/200 [00:25<00:16,  4.84it/s]\n",
      "Training MLP:  41%|████      | 26/64 [10:45<16:21, 25.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  62%|██████▏   | 123/200 [00:24<00:15,  4.98it/s]\n",
      "Training MLP:  42%|████▏     | 27/64 [11:10<15:43, 25.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  49%|████▉     | 98/200 [00:20<00:21,  4.67it/s]\n",
      "Training MLP:  44%|████▍     | 28/64 [11:31<14:29, 24.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|█████▌    | 110/200 [00:24<00:19,  4.52it/s]\n",
      "Training MLP:  45%|████▌     | 29/64 [11:55<14:07, 24.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  56%|█████▌    | 111/200 [00:23<00:18,  4.75it/s]\n",
      "Training MLP:  47%|████▋     | 30/64 [12:19<13:35, 23.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  68%|██████▊   | 136/200 [00:30<00:14,  4.51it/s]\n",
      "Training MLP:  48%|████▊     | 31/64 [12:49<14:12, 25.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  52%|█████▏    | 103/200 [00:22<00:20,  4.64it/s]\n",
      "Training MLP:  50%|█████     | 32/64 [13:11<13:11, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  54%|█████▍    | 108/200 [00:23<00:19,  4.66it/s]\n",
      "Training MLP:  52%|█████▏    | 33/64 [13:34<12:32, 24.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  59%|█████▉    | 118/200 [00:24<00:16,  4.87it/s]\n",
      "Training MLP:  53%|█████▎    | 34/64 [13:59<12:08, 24.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  52%|█████▎    | 105/200 [00:23<00:21,  4.51it/s]\n",
      "Training MLP:  55%|█████▍    | 35/64 [14:22<11:35, 23.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████▌   | 130/200 [00:27<00:14,  4.68it/s]\n",
      "Training MLP:  56%|█████▋    | 36/64 [14:50<11:43, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  76%|███████▌  | 152/200 [00:31<00:09,  4.89it/s]\n",
      "Training MLP:  58%|█████▊    | 37/64 [15:21<12:07, 26.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|████▉     | 99/200 [00:20<00:21,  4.75it/s]\n",
      "Training MLP:  59%|█████▉    | 38/64 [15:42<10:53, 25.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 101/200 [00:20<00:20,  4.86it/s]\n",
      "Training MLP:  61%|██████    | 39/64 [16:03<09:55, 23.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  39%|███▉      | 78/200 [00:16<00:25,  4.85it/s]\n",
      "Training MLP:  62%|██████▎   | 40/64 [16:19<08:36, 21.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 100/200 [00:21<00:21,  4.67it/s]\n",
      "Training MLP:  64%|██████▍   | 41/64 [16:40<08:14, 21.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|█████▍    | 109/200 [00:22<00:18,  4.80it/s]\n",
      "Training MLP:  66%|██████▌   | 42/64 [17:03<08:00, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  46%|████▌     | 92/200 [00:19<00:23,  4.64it/s]\n",
      "Training MLP:  67%|██████▋   | 43/64 [17:23<07:26, 21.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  66%|██████▋   | 133/200 [00:27<00:13,  4.89it/s]\n",
      "Training MLP:  69%|██████▉   | 44/64 [17:50<07:40, 23.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  69%|██████▉   | 138/200 [00:27<00:12,  4.96it/s]\n",
      "Training MLP:  70%|███████   | 45/64 [18:18<07:45, 24.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  61%|██████    | 122/200 [00:25<00:16,  4.72it/s]\n",
      "Training MLP:  72%|███████▏  | 46/64 [18:44<07:28, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  63%|██████▎   | 126/200 [00:27<00:15,  4.63it/s]\n",
      "Training MLP:  73%|███████▎  | 47/64 [19:11<07:15, 25.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|███████▉  | 159/200 [00:32<00:08,  4.83it/s]\n",
      "Training MLP:  75%|███████▌  | 48/64 [19:44<07:24, 27.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████▌   | 130/200 [00:26<00:14,  4.86it/s]\n",
      "Training MLP:  77%|███████▋  | 49/64 [20:11<06:52, 27.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|█████▉    | 119/200 [00:23<00:16,  4.97it/s]\n",
      "Training MLP:  78%|███████▊  | 50/64 [20:35<06:10, 26.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  53%|█████▎    | 106/200 [00:22<00:19,  4.71it/s]\n",
      "Training MLP:  80%|███████▉  | 51/64 [20:57<05:28, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  54%|█████▎    | 107/200 [00:23<00:20,  4.46it/s]\n",
      "Training MLP:  81%|████████▏ | 52/64 [21:21<04:58, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  61%|██████    | 122/200 [00:27<00:17,  4.49it/s]\n",
      "Training MLP:  83%|████████▎ | 53/64 [21:48<04:41, 25.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "all_datasets = [\n",
    "    (\"california\", \"regression\"),\n",
    "    (\"otto\", \"classification\"),\n",
    "    (\"telcom\", \"classification\"),\n",
    "    (\"mnist\", \"classification\"),\n",
    "]\n",
    "for dataset_name, task_type in all_datasets:\n",
    "    train_ds, val_ds, test_ds = load_dataset(dataset_name=dataset_name)\n",
    "    train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=config.batch_size, shuffle=False)\n",
    "    for hidden_dim in config.hidden_dims:\n",
    "        print(f\"\\nDataset: {dataset_name}, Hidden_dim: {hidden_dim}\")\n",
    "        if hidden_dim in [64, 128]:\n",
    "            second_nfix = 3\n",
    "        else:\n",
    "            second_nfix = 4\n",
    "        mask_params = {\n",
    "            0: {'mask_constant': 1, 'mask_type': config.mask_type, 'do_normal_mask': True, 'num_fixed': 2},\n",
    "            1: {'mask_constant': 1, 'mask_type': config.mask_type, 'do_normal_mask': True, 'num_fixed': second_nfix},\n",
    "            2: {'mask_constant': 1, 'mask_type': config.mask_type, 'do_normal_mask': True, 'num_fixed': second_nfix},\n",
    "            3: {'mask_constant': 1, 'mask_type': config.mask_type, 'do_normal_mask': True, 'num_fixed': second_nfix},\n",
    "        }\n",
    "        in_dim = train_ds.tensors[0].shape[1]\n",
    "        if task_type == \"regression\":\n",
    "            out_dim = 1\n",
    "            criterion = nn.MSELoss()\n",
    "        else:\n",
    "            out_dim = len(torch.unique(train_ds.tensors[1]))\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for rep_i in range(config.repeats):\n",
    "            print(f\"\\nRepetition {rep_i + 1}/{config.repeats}\")\n",
    "            current_seed = config.base_seed + rep_i * 10000\n",
    "\n",
    "            mlp_metrics = []\n",
    "            wmlp_metrics = []\n",
    "            wmlp_masked_ratios = []\n",
    "            mlp_models = []\n",
    "            wmlp_models = []\n",
    "\n",
    "            for i in tqdm(range(config.total_models), desc=\"Training MLP\"):\n",
    "                seed_value = current_seed + i\n",
    "                set_global_seed(seed_value)\n",
    "\n",
    "                mlp = MLP(in_dim, hidden_dim, out_dim, num_layers=4, norm=None)\n",
    "                optimizer = torch.optim.AdamW(mlp.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "                mlp, train_time, train_losses, val_losses = train_one_model(\n",
    "                    mlp, train_loader, val_loader, criterion, optimizer,\n",
    "                    device=config.device, max_epochs=config.max_epochs, patience=config.patience\n",
    "                )\n",
    "                mlp.to(\"cpu\")\n",
    "                mlp_models.append(copy.deepcopy(mlp))\n",
    "\n",
    "                test_metric = evaluate_model(mlp, test_loader, criterion, config.device, task_type=task_type)\n",
    "                mlp_metrics.append(test_metric)\n",
    "\n",
    "            for i in tqdm(range(config.total_models), desc=\"Training WMLP\"):\n",
    "                seed_value_wmlp = current_seed + 2000 + i\n",
    "                set_global_seed(seed_value_wmlp)\n",
    "\n",
    "                wmlp = WMLP(in_dim, hidden_dim, out_dim, num_layers=4, mask_params=mask_params, norm=None)\n",
    "                optimizer_wmlp = torch.optim.AdamW(wmlp.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "                wmlp, train_time_wmlp, train_losses_w, val_losses_w = train_one_model(\n",
    "                    wmlp, train_loader, val_loader, criterion, optimizer_wmlp,\n",
    "                    device=config.device, max_epochs=config.max_epochs, patience=config.patience\n",
    "                )\n",
    "                wmlp.to(\"cpu\")\n",
    "                wmlp_models.append(copy.deepcopy(wmlp))\n",
    "                test_metric_wmlp = evaluate_model(wmlp, test_loader, criterion, config.device, task_type=task_type)\n",
    "                wmlp_metrics.append(test_metric_wmlp)\n",
    "\n",
    "                ratio, masked = wmlp.report_masked_ratio()\n",
    "                wmlp_masked_ratios.append(ratio)\n",
    "\n",
    "            avg_dist_mlp = average_pairwise_distance(mlp_models)\n",
    "            avg_dist_wmlp = average_pairwise_distance(wmlp_models)\n",
    "\n",
    "            avg_wmlp_masked_ratio = float(np.mean(wmlp_masked_ratios)) if wmlp_masked_ratios else 0.0\n",
    "\n",
    "            ensemble_results_mlp = {}\n",
    "            ensemble_results_wmlp = {}\n",
    "            for ens_size in config.ensemble_sizes:\n",
    "                mlp_sub = mlp_models[:ens_size]\n",
    "                wmlp_sub = wmlp_models[:ens_size]\n",
    "                ens_metric_mlp = evaluate_ensemble(mlp_sub, test_loader, config.device, task_type=task_type)\n",
    "                ens_metric_wmlp = evaluate_ensemble(wmlp_sub, test_loader, config.device, task_type=task_type)\n",
    "                ensemble_results_mlp[ens_size] = ens_metric_mlp\n",
    "                ensemble_results_wmlp[ens_size] = ens_metric_wmlp\n",
    "\n",
    "            log_dict = {\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"task_type\": task_type,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"repeat_index\": rep_i + 1,\n",
    "                \"avg_dist_mlp\": avg_dist_mlp,\n",
    "                \"avg_dist_wmlp\": avg_dist_wmlp,\n",
    "                \"avg_wmlp_masked_ratio\": avg_wmlp_masked_ratio,\n",
    "                \"mean_mlp_metric\": float(np.mean(mlp_metrics)),\n",
    "                \"mean_wmlp_metric\": float(np.mean(wmlp_metrics)),\n",
    "\n",
    "            }\n",
    "\n",
    "            for ens_size in config.ensemble_sizes:\n",
    "                log_dict[f\"mlp_ens_{ens_size}\"] = ensemble_results_mlp[ens_size]\n",
    "                log_dict[f\"wmlp_ens_{ens_size}\"] = ensemble_results_wmlp[ens_size]\n",
    "\n",
    "            wandb.log(log_dict)\n",
    "\n",
    "            print(f\"Repetition {rep_i + 1}/{config.repeats} finished.\")\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asym-ensembles-Bo8XDwk--py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
